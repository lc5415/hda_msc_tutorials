{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this tutorial, we will be looking at implementing the Decision Tree tools that are avaiable from scikit-learn. In particular we will be looking applying a simple decision tree, using the Gini Index, to the Iris dataset, followed by a more complex dataset. Both are classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preamble\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics\n",
    "\n",
    "import graphviz \n",
    "\n",
    "# Helper function to plot the decision tree. This uses the graphviz library.\n",
    "def plot_tree(graph, feature_names=None, class_names=None):\n",
    "    '''\n",
    "    This method takes a DecisionTreeClassifier object, along with a list of feature names and target names\n",
    "    and plots a tree. The feature names and class names can be left empty; they are just there for labelling \n",
    "    '''\n",
    "    dot_data = export_graphviz(graph, out_file=None, \n",
    "                      feature_names=feature_names,  \n",
    "                      class_names=class_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True) \n",
    "    \n",
    "    graph = graphviz.Source(dot_data)\n",
    "    \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the iris dataset using the helper function provided by scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Note the prefix 'iris' here. This is so that we don't get confused with the data for the second dataset later on.\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can create a DecisionTreeClassifier object from scikit-learn. This has several options that we can change, but for now, let's see what the default tree looks like for the full iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1) We initialise the DecisionTreeClassifier object. We can set our hyperparameters here if necessary; we'll look into this in a bit.\n",
    "base_model = DecisionTreeClassifier()\n",
    "\n",
    "# B2) DecisionTreeClassifier has a fit method, which takes the train X and train y to learn a tree. The necessary optimisation is done here. \n",
    "fitted_base_model = base_model.fit(iris_X, iris_y)\n",
    "\n",
    "# B3) Use the helper function defined above to plot the learned model.\n",
    "plot_tree(fitted_base_model, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as per the [documentation](https://scikit-learn.org/stable/modules/tree.html), DecisionTreeClassfier implements a modified version of CART. You can look at documentation for details about the particular algorithm scikit-learn implements. \n",
    "\n",
    "Since it implements CART, by default, DecisionTreeClassifier uses the Gini Index to measure the leaf impurity. You can also use the entropy information gain by setting `criterion='entropy'` when initialising the object. \n",
    "\n",
    "Other options available can be seen [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). A key option is the `max_depth`. This defines the maximum depth of the tree. If this isn't set, as above, the algorithm will continue until all leaves are pure, or until all leaves contain less than `min_samples_split` samples; this is another setting we can set. \n",
    "\n",
    "In other words, `max_depth` reduces the depth of the full tree seen above until the longest path in the tree is equal to `max_depth`. Pruning simplifies the model and makes it more interpretable to humans, and also prevents overfitting. \n",
    "\n",
    "Lets look at how the `max_depth` affects the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test-train data split. \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We have set the random seed to be 2, by setting the random_state parameter. \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Initialise a new model that uses the default `max_depth`. The code pattern is the same as above.\n",
    "\n",
    "# Fill in the blanks below.\n",
    "# B4) Create a DecisionTreeclassifier object with the correct hyperparameters. This is your model.\n",
    "\n",
    "\n",
    "# B5) Use the DecisionTreeclassifier.fit(X, y) function to optimise the model.\n",
    "\n",
    "\n",
    "# B6) Use the DecisionTreeclassifier.predict(X) to make predictions\n",
    "\n",
    "\n",
    "# B7) Use metrics.accuracy_score(y_test, y_predictions) to compute accuracy scores. Print it.\n",
    "full_model_accuracy = \n",
    "\n",
    "print(f'Accuracy: {full_model_accuracy}')\n",
    "\n",
    "# B8) Plot the tree using plot_tree(model, feature_names, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise another model that sets `max_depth=3`. The code pattern is the same as above.\n",
    "\n",
    "# B4)\n",
    "\n",
    "# B5)\n",
    "\n",
    "# B6)\n",
    "\n",
    "# B7)\n",
    "small_model_accuracy = \n",
    "\n",
    "print(f'Accuracy: {small_model_accuracy}')\n",
    "# B8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that our accuracy has improved, albeit slightly.\n",
    "Note however, that this is dependent on the random seed that we used (since we only ran this once).\n",
    "You can try changing the random_state parameter above when we used the train_test_split function, and see if the result above changes.\n",
    "\n",
    "We can now do a more extensive search by doing a grid search to search over several `max_depth` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try using the GridSearchCV from sklearn.model_selection. \n",
    "# You can specifiy the set of `max_depth`s that you want to try by setting `param_grid={'max_depth':[1, 2, 3, 4, 5, 6]}`.\n",
    "# Documentation can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The code pattern here is similar to the previous sections. \n",
    "# G1) Initiate a GridSearchCV object with the correct model, param_grid, and cv; `cv=k` does a k-fold cross-validation.\n",
    "grid_search_model = GridSearchCV(DecisionTreeClassifier(random_state=2), {'max_depth':[1, 2, 3, 4, 5, 6]}, cv=15,)\n",
    "\n",
    "# G2) use the GridSearchCV.fit(X, y) method to run the grid search with cv. \n",
    "fitted_grid_search_model = grid_search_model.fit(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean accuracy scores. \n",
    "# The fitted GridSearchCV object has an attribute model.cv_results_ (note the underscore) that gives us a dict object with several results from the cross-valiation. \n",
    "# In particular, one of the key value pairs avaiable is 'mean_test_score', which returns the average score for each parameter value over the cv fold. \n",
    "\n",
    "accuracy_scores = fitted_grid_search_model.cv_results_['mean_test_score']\n",
    "print(f\"Mean accurary scores:{accuracy_scores}\")\n",
    "\n",
    "# Plot the best estimator you found\n",
    "# GridSearchCV.best_estimator_ (again, the underscore) returns the model that performed the best. This behaves the same as the model objects from before, so we can plot it.\n",
    "# G3) Get the best model\n",
    "best_tree_model = fitted_grid_search_model.best_estimator_\n",
    "\n",
    "# G4) Plot the best model\n",
    "plot_tree(best_tree_model, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIMA-INDIAN Dataset\n",
    "Now lets do the same analysis for a much more complicated dataset. Details about this dataset can be found [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database). As a summary, it contains 768 data points with 8 features and a single binary target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "\n",
    "# load dataset. Here we are loading into Pandas, since the data is available as a csv file.\n",
    "# The DecisionTreeClassifier object can accept dataframes when fitting. \n",
    "pima = pd.read_csv(\"diabetes.csv\", names=col_names)\n",
    "\n",
    "print(pima.head())\n",
    "\n",
    "pima_feature_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age']\n",
    "pima_class_names = ['True', 'False']\n",
    "\n",
    "pima_X = pima[pima_feature_names]\n",
    "pima_y = pima.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, lets look at what the tree looks like with default settings.\n",
    "# Follow the example in Cell 3 above, where we did something similar on the Iris Dataset.\n",
    "\n",
    "# B1)\n",
    "\n",
    "# B2)\n",
    "\n",
    "# B3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that this is quite unwieldy. \n",
    "\n",
    "Lets immediately jump into see do the grid search, and see if we can do better with a smaller tree. \n",
    "\n",
    "Note from the previous cell that the depth of this graph is 14, so the highest depth we need to look at is 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Follow code in a similar pattern as when we did grid search on the iris dataset in Cell 6. \n",
    "\n",
    "# G1)\n",
    "\n",
    "# G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean accuracy scores. \n",
    "# As before we can use GridSearchCV.cv_results_['mean_test_score'] for this.\n",
    "# Remember to run %matplotlib inline if you are using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best estimator you found\n",
    "# G3)\n",
    "\n",
    "# G4)\n",
    "\n",
    "# As we can see, this is much more interpretable (at least compared to the previous result), and also has a better accuracy score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hda_msc_tutorials",
   "language": "python",
   "name": "hda_msc_tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
