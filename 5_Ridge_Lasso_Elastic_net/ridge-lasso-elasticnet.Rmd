---
title: "Ridge, Lasso and Elastic Net"
author: "Adapted from Seth Flaxman"
date: "17/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, you will familiarize yourself with the maths underlying the ridge and lasso penalties. Then you will 
write code to investigate bias and variance in the context of ridge, lasso, and the elastic net. 
Finally you will investigate Principal Components Analysis (PCA) as an alternative dimensionality reduction method.

Make sure the ``glmnet`` package is installed:
```{r}
library(glmnet)
```
## Ridge and lasso 
Let us considered squared error loss, i.e. in linear regression.
The ridge penalty adds an $L_2$ norm to a loss function: 
$\sum (y_i - x_i\beta)^2 + \lambda \|\beta\|^2_2$

The lasso penalty adds an $L_1$ norm to a loss function:
$\sum (y_i - x_i\beta)^2 + \lambda |\beta|_1^1$

The loss function for ridge leads to the following optimization problem:

$\mbox{argmin}_{\beta} \sum (y_i - x_i\beta)^2 + \lambda \|\beta\|^2_2$

* Assume that $\beta \in R^p$ is a p-dimensional vector and rewrite the loss function with the $L_2$ norm using the vector elements
$\beta_1$, $\beta_2, \ldots, \beta_p$.

* Similarly, write the loss function for the lasso using vector elements.

* To minimize these loss functions, we need to find their gradients. Find the gradient of the ridge penalized loss function. What parameter are you taking the gradient with respect to?

## Bias vs. variance
Ridge and lasso introduce *bias* by shrinking the parameters in a model towards zero. Here is the code from the unbiased linear regression demo in class:

```{r}
result = NULL
for(i in 1:1000) {
  n = 50
  x = rnorm(n)
  u = seq(-5,5,.1)
  error = rnorm(n) * .5
  y = 1.5 * x + error
  yhat = function(beta,x) {
    return(beta * x)
  }
  squared.loss = function(residuals) {
    sum(residuals^2)
  }
  
  betahat = optimize(function(beta) squared.loss(y - yhat(beta,x)),interval=c(-1,3))$minimum
  
  result = rbind(result,data.frame(beta.optim = betahat, true.beta = 1.5))
}
hist(result$beta.optim,breaks=25)
abline(v=mean(result$beta.optim),col="blue",lwd=2)
abline(v=1.5,col="red",lwd=2)
mean(result$beta.optim)
var(result$beta.optim)
```

* Modify the code to include a ridge penalty. Consider a range of $\lambda$ values from 0 to 10. What do you notice?
* Modify the code to include a lasso penalty.  Consider a range of $\lambda$ values from 0 to 10. What do you notice?
* Earlier you found the derivatives of the ridge and lasso penalties. How does each vary with $\beta$? What does this mean in terms of how much each penalty penalizes large vs. small values of $\beta$?

## Mouse genome dataset example with ridge, lasso, and the elastic net

Ridge and lasso decrease *variance* by making a model less complex. We will consider a mouse genome dataset to predict red blood cell count from SNPs.
You can read about
the dataset at https://www.sanger.ac.uk/science/data/mouse-genomes-project 

The dataset has 1522 observations and 10346 predictors. I've provided you with an 80% train and 10% test dataset. (I'll release the remaining
10% on Thursday.) If we consider just linear regression,
the problem is overdetermined, and we can find an infinite number of models which perfectly fit the data:

```{r}
load("mice.rdata") # loads X_train, X_test, y_train, y_test
# there are computational difficulties in addition to theoretical ones with fitting a model with so many predictors, so we randomly sample 1300
set.seed(1)
ii = sample(ncol(X_train),1300)
X_train_df = as.data.frame(X_train[,ii])
X_train_df$y = y_train
X_test_df = as.data.frame(X_test[,ii])
fit = lm(y ~ ., data=X_train_df)
plot(y_train,predict(fit)) # an almost perfect fit
plot(y_test,predict(fit,X_test_df)) # very bad fit---this is because our model has high variance
```

* Inspect the coefficients in the linear model. You can find their values using ``coef(fit)``. Do you notice any that are very large in magnitude?

Now let us see how lasso and ridge serve to decrease variance by shrinking all coefficients (ridge and lasso) and zeroing some of them out (lasso).
Here is how to fit the elastic net (a mixture of ridge and lasso)
in the ``glmnet`` package. We will now consider all 10346 predictors.
```{r}

# I have set the lambda sequences for illustration, but glmnet can cleverly determine these for you, so in practice, it is better to leave out the lambda parameter, i.e. fit.ridge=glmnet(X,y,alpha=0)
fit.ridge = glmnet(data.matrix(X_train),y_train,alpha=0,lambda = seq(100,.01,length.out = 100)) 
fit.lasso = glmnet(data.matrix(X_train),y_train,alpha=1,lambda=seq(2,.01,length.out=100)) 
fit.elastic = glmnet(data.matrix(X_train),y_train,alpha=.5,lambda=seq(2,.01,length.out=100)) 

par(mfrow=c(1,3))
plot(fit.ridge,xvar="lambda")
plot(fit.lasso,xvar="lambda")
plot(fit.elastic,xvar="lambda")
```

* Use ``predict`` to get predictions. If you call:
```
yhat = predict(fit.ridge,data.matrix(X_test))
```
you will get a matrix of predictions corresponding to the predictions made at each point in the regularization path. 
If you allowed  ``glmnet`` to choose the values of $\lambda$, you can check them by typing:
```
fit.ridge$lambda
```

* Calculate a loss function (which one is appropriate for a linear model?) at each value of $\lambda$ for lasso, ridge, and the elastic net, using
the training data. Repeat for the testing data. Plot both of these curves---these are the training and testing learning curves.

* What values of $\lambda$ give solutions that approach that of ordinary linear regression (without regularization)? For lasso, what
value of $\lambda$ corresponds to a model with no predictors? What are the predictions for this model?