{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure:  \n",
    "    1. Load data  \n",
    "    2. Perform PCA and check eigenvalues  \n",
    "    3. Perform k-means with different values of k  \n",
    "    4. Kernel versions of PCA and k-means  \n",
    "    5. Convolutional neural networks for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X = pd.DataFrame(dataset['data'])\n",
    "y = pd.DataFrame(dataset['target'])\n",
    "y.columns = ['labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Perform PCA and plot the first two principal components\n",
    "\n",
    "We can preprocess the data so that they are standardised. scikit-learn has a preprocessing module that has a [`sklearn.preprocessing.StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which we can use.\n",
    "\n",
    "We will then use the [`sklearn.decomposition.PCA()`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to carry out PCA. \n",
    "An important parameter that can be passed in when initialising the model is `n_components`. As you can guess, this will set the number of principal components we want.\n",
    "\n",
    "`sklearn.decomposition.PCA()` has the same pattern as most sklearn models, so we can fit our data to it by doing PCA.fit(X=data). \n",
    "\n",
    "This model also has a `PCA.fit_transform(X=data)` which does the fit, and outputs the transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features to zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA with sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Do PCA for n_components = 30.\n",
    "\n",
    "\n",
    "# Calculate the projected datapoints. You can use PCA.fit_transform(X=data) to do this directly.\n",
    "principalComponents = \n",
    "\n",
    "# Extract the projections of the first 2 components\n",
    "principalComponents_2 = principalComponents[:, :2]\n",
    "\n",
    "\n",
    "# Plot the first 2 components\n",
    "principalDf = pd.DataFrame(data = principalComponents_2, columns = ['principal component 1', 'principal component 2'])\n",
    "# Merge with labels\n",
    "finalDf = pd.concat([y, principalDf], axis = 1)\n",
    "finalDf.column = ['labels', 'principal component 1', 'principal component 2']\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "ax = sns.scatterplot(x=\"principal component 1\", y=\"principal component 2\", hue=\"labels\",\n",
    "                     data=finalDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create a scree plot of the principal components\n",
    "\n",
    "A [scree plot](https://en.wikipedia.org/wiki/Scree_plot) shows how some 'explanatory' factor varies with the component number. Here we will use [`PCA.explained_variance_`](https://eranraviv.com/understanding-variance-explained-in-pca/) to show us the amount of variance that is explained by each of the components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot component number vs explained_variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Calculate and print the eigenvalues \n",
    "Are the projected values as you would expect?\n",
    "\n",
    "You can manually compute the covariance matrix using `numpy` with its `np.cov()` method. Keep in mind that we are interested in the variability of the features. Note that you might have to transpose your data matrix.\n",
    "\n",
    "Eigenvalues and eigenvectors can be calculated using `np.linalg.eig(cov_matrix)`. This returns a tuple `(eig_vals, eig_vec)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix.\n",
    "\n",
    "# Compute the eigenvectors\n",
    "eig_vals, eig_vecs =\n",
    "\n",
    "print(eig_vals)\n",
    "\n",
    "# Do you notice something about the values of the eig_vals and the explained_variance from above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Perform K-means clustering for different values of K and plot the resulting clusters\n",
    "\n",
    "Scikit learn has a model for this: [`sklearn.cluster.KMeans()`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). The key parameter we can adjust here is the number of clusters, set by the value of `num_clusters`.\n",
    "\n",
    "As with `sklearn.decomposition.PCA()`, `sklearn.cluster.KMeans()` has a method `KMeans.fit_predict` which runs the fitting and outputs the center of the clusters our data points belong to. \n",
    "\n",
    "Lets look at how changing this affects our clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create some data\n",
    "n_samples = 1500\n",
    "\n",
    "title='Normal'\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=7)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(f'{title}: original')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_kmeans_n(n=2, data_set=X):\n",
    "    # Initiate a KMeans() model that takes in num_clusters as a variable n.\n",
    "\n",
    "    \n",
    "    # Predict the centers of the datasets using fit_predict(data_set) that takes in the variable data_set.\n",
    "\n",
    "    y_pred = \n",
    "    # Plot\n",
    "    plt.scatter(data_set[:, 0], data_set[:, 1], c=y_pred)\n",
    "    plt.title(f'{title} with N={n}')\n",
    "    plt.show()\n",
    "\n",
    "# Try running this function for different values of n\n",
    "plot_kmeans_n(10, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try a data set that is not as clearly separated.\n",
    "title='High Variance'\n",
    "X_var, y_var = make_blobs(n_samples=n_samples, \n",
    "                  cluster_std=[2.5, 2.5, 2.5],\n",
    "                  random_state=7)\n",
    "\n",
    "plt.scatter(X_var[:, 0], X_var[:, 1], c=y_var)\n",
    "plt.title(f'{title}: original')\n",
    "plt.show()\n",
    "\n",
    "# Run the same function we defined above, but set data_set=X_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Kernel PCA. Using the data generated below, apply kernel PCA and plot the principal components\n",
    "\n",
    "As you can imagine, sklearn has a model called [`sklearn.decomposition.KernalPCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html), which also has a `fit_transform` method, similar to `sklearn.decomposition.PCA()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Create a dataset that is not easily separable.\n",
    "X_circle, y_circle = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "plt.scatter(X_circle[:, 0], X_circle[:, 1], c=y_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at what we get if we apply regular PCA first\n",
    "# Run PCA on the X_circle data and get the projected data.\n",
    "\n",
    "\n",
    "\n",
    "X_pca = \n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same techniques in PCA, but with using KernelPCA instead\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "X_kpca = kpca.fit_transform(X_circle)\n",
    "\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y_circle)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Try changing the value of gamma to see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do an inverse transform on the projected data using KernalPCA.inverse_transform(projected_data).\n",
    "# Note that PCA cannot capture all the variance. See what happens.\n",
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "\n",
    "plt.scatter(X_back[:, 0], X_back[:, 1], c=y_circle)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    https://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set\n",
    "    https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hda",
   "language": "python",
   "name": "hda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
